**SENG 438- Software Testing, Reliability, and Quality**

**Lab. Report \#5 – Software Reliability Assessment**

| Group 23:       |   |
|-----------------|---|
| Jennifer Jay    |   |
| Nicole Heather  |   |
| Nora Melik      |   |
| Creek Thompson  |   |

# Introduction

Software reliability and quality are the parts of the final steps of testing for an application. As a result, we are now shitfting away from unit testing with source code to focusing on the deployment and integration of program. Through the use of these reliability testing tools we can experiment with different models and metrics in order to properly determine if our SUT meets our desired requirements. This kind of testing however is not just for the developers themsevles, but also allows stakeholders and purchasers to undertsand and have input on the kind of thresholds in place for the program. As a result reliability and quiality testing are major parts of testing within software, and provide insights into the determination and risk assesment of deploying a product.

# Assessment Using Reliability Growth Testing 

Result of model comparison (selecting top two models):

Geometric and Littlewood and Varral's Bayesian Reliability were used for the Time Between Failures, and Schneidewind’s Maximum Likelihood and Yamada’s S-Shaped Reliability Growth were used for Failure Count. For Time Between Failures, Littlewood and Varral's Bayesian Reliability fits the data better than Geometric, as the blue line of fit follows the red data points more accurately, indicating that out of the two models, Littlewood and Varral's Bayesian Reliability was more successful in learning and plotting the data. For Failure Count, the models chosen do not reflect a line of best fit, making it more difficult to analyze which model is a better algorithm for the Failure Count data. However, the Schneidewind’s Maximum Likelihood provides more calculated parameters that represent values that can be used to identify different characteristics of the dataset, whereas Yamada’s S-Shaped Reliability Growth only provides Estimate values and Prediction. These models in general may not have been the best possible outcome for representing the data, as they lacked a line of best fit, but they were the only available models in the SRTAT that successfully outputted information when run. Due to more metrics being given for the Schneidewind’s Maximum Likelihood model, it would be the better fit out of the two for Failure Count. Therefore, the selection of the top two models would be Littlewood and Varral's Bayesian Reliability for Time Between Failures and Schneidewind’s Maximum Likelihood for Failure Count.

Result of range analysis (an explanation of which part of data is good for proceeding with the analysis):

To find a suitable range for analysis, it is important to reject outliers that cause deviation from the overall trend of the data to avoid skewing analysis results. In the Geometric analysis graph, we would reject the 3 points above time 4000 and before failure number 110 as they clearly deviate from the exponentially increasing trend of the data. The other points relatively fit the trend so restricting a time period is not crucial. For the Littlewood and Varral's Bayesian Reliability graph, we would remove the 2 points above time 7,000 and a point roughly at (86, 4150) since those deviate the most from the trend. We would limit the time period to start after failure number 5 since there are a couple points prior that skews the beginning of the exponential trend to start closer to time 1,000 when most of the points are closer to time 0. For Schneidewind’s Maximum Likelihood and Yamada’s S-Shaped Reliability Growth graphs, we would reject the outliers above time 7.

Plots for failure rate and reliability of the SUT for the test data provided:



A discussion on decision making given a target failure rate:

If the observed failure rate does not meet a given target failure rate, we need to decide how feasible it would be to close the gap. A decision should be made based on constraints of resources, time and technical capabilities. The risks of accepting the current failure rate should also be accounted for. If the observed failure rate does meet the failure rate, a discussion on the implications of proceeding with the current system’s reliability should be made.

A discussion on the advantages and disadvantages of reliability growth analysis:

Reliability growth analysis allows users to assess improvements and identify weaknesses through performance tracking so that they can enhance the system based on targeted areas needed for improvement. However, reliability growth data heavily requires failure data. Failure data must be available with limited bias to avoid inaccurate analysis. The analysis also assumes that the system will continuously improve in reliability, however, this is not always the case if underlying issues of the system remain unresolved.

# Assessment Using Reliability Demonstration Chart 

3 plots for MTTFmin, twice and half of it for your test data:

For Dataset CSR2.DAT:

- Standard MTTFmin:

![alt text](Images/Standard_CSR.png)

- Double MTTFmin:

![alt text](Images/Double_CSR.png)

- Half MTTFmin:

![alt text](Images/Half_CSR.png)

For Dataset J1.DAT:

- Standard MTTFmin:

![alt text](Images/Standard_J1.png)

- Double MTTFmin:

![alt text](Images/Double_J1.png)

- Half MTTFmin:

![alt text](Images/Half_J1.png)


Explain your evaluation and justification of how you decide the MTTFmin:

CSR2.DAT:
- Evaluation of Result:
 - MTTFmin Justification: 
    The original dataset lacks a specific time unit for its time-between values, however this is ignorable since they are normalized when plotted into the RDC. First we start with a reasonable assumption on the usage and up-time for MTTF, that way we can cover for a standard, very high and very low MTTFmin when re-calculating and re-making the graph. We must also take into account the tool used and readability when evaluating the output, a value that is too high or too low makes the result difficult to undertsand. For this failure dataset we can assume that each time unit can be equated to a second (for the sake of using SRTAT) and that we have system that functions reasonably. 
    
    From looking at our provided data, our system ran for a total of 61,316 time units, with a total of 129 failures. So lets say that our starting FIO is 129 / 61,316 = 0.002 failures / second. Taking into account the data from our report helps keep the persepective of how the system actually functioned, instead of needing to assume off of no knowledge. This means our MTTFmin is 1.5, doubling it gives; MTTFmin * 2 = 3 and dividing it gives; MTTFmin / 2 = 0.75.

J1.DAT:
- Evaluation of Result:
- MTTFmin Justicfication:
    Very similarly to the previous dataset, we lack any specific time units to use as a benchmark. So we will be approaching this data set in the same way. This includes taking into account the data that we already have, and ensuring that we still make reasonable assumptions so that evaluating the resulting charts are made clear. For the sake of consistancy, we are not changing any risk values or discrimination factors. 

    The failure count data gives us a slighly different view into the functioning of this system. The time intervals are always set in singles slices, whereas the counting of failures resets each time. After running for a total of 62 time units, we have a total of 150 failures. This leaves us with a failure rate of 150 / 62 = 2.4 failures per time unit. Setting this as our FIO, we have a MTTFmin value of around 14. Now much like before we can both double (MTTFmin * 2 = 28), and half this (MTTFmin / 2 = 7). 

A discussion on the advantages and disadvantages of RDC:

- Advantages: RDC is an efficient way to provide information on the performance and reliability of a system, it is also easily understood as the chart itself makes very clear what the data represents. This method also allows for input of not just the developers, but any stakeholders as well. This rask managements is flexible, allowing multiple changes to both the accept and reject lines as well as the ability to experiment and compare MTTFmin and FIO values. Finally the data we used for the creation of these charts was minimal, meaning that we do not need an a large amount of data to analyze our system as the chart makes the visualization relativley easy. 

- Disadvantages: RDC is a tool that only able to tell us if a system meets an acceptable level of requirements. The information is provides us is not exact, and can only give a more simplified overview of te SUT. Another issue is the lack of specific sytem information. For example, the data used for the charts above lacked any specific data regarding the kinds of failiures nor how these failures happened. This could be serious issue in an actual development enviroment where we need to fix these issues in order for the system to meet requiremtns. RDC in no way can tell us specifically where we can improve our code, only that we meet our specifications or not.

# Comparison of Results

Part 2: 
The results for Part 1 involved finding and comparing the top two models for each dataset by using the SRTAT tool, where Geometric and Littlewood and Varral's Bayesian Reliability were the models applied to the Time Between Failures data, and the Schneidewind’s Maximum Likelihood and Yamada’s S-Shaped Reliability Growth models were applied to the Failure Count. As analyzed in Part 1, it was found that Littlewood and Varral's Bayesian Reliability was the better model for Time Between Failures, and for Failure Count, it was found that Schneidewind’s Maximum Likelihood was the better model. The range analysis also served in inspecting the actual data used for these models and how removing outliers could improve the models’ behavior. 

Part 3:


Comparison:

# Discussion on Similarity and Differences of the Two Techniques

Part 2:

Part 3:

Comparison:

# How the team work/effort was divided and managed

The planning of the stages where all done together, individually each member had some times to familiarize themsevles with the specific tools being used. Then all member met together to follow though on the execution and analysis of the ouput of our reliablility testing tools. This was done all together unlike the previous labs because of the lack of coding required, as well as wanting the analysis and comparison to be freely disscused and argued among everyone.

As for the lab report itself, all members contirbuted to each section in some way. This includes writing, editing and quality assurance. This way we could have a consistant report while still distributing work around evenly between each member. 

# Difficulties encountered, challenges overcome, and lessons learned

Originally, there where some issues in regards to the functioning of the tools used. This resulted in more time needing to be taken to get used to our tools as well a figure out how to approach the analysis and comparison of our results.

However this lab was an insightful look into the uses and procedure of both reliability growth testing and the demonstration chart. Being able to look at the many different uses and models used for the visualization of reliability and quality was benificial in our understanding of software testing.

# Comments/feedback on the lab itself

This lab had some issues with the instructions and tools given. Although followable, the instructions in the lab document lacked any details. It is difficult, especially for a subjective task, to ensure that the process of choosing models, interpreting graphs and setting new values are all done somwhat correctly. It would help to have a more concrete explanation within the lab itself than have to read over the large amount of documentation given for each tool we might use, as it is hard to discern what information is relevant.\

There is also some issues with the tool provided themselves. These tools look very different than what was provided in the document, and the data that was given in order to complete the section for part 1 was unusable. We where able to test and learn how to use these tool using the sample data provided, but that data was not the target for this lab. There is also an issue with the computing time and errors with SRTAT specifically, where some models and formulas gave unuexplanable errors, or would execute seemingly forever and needed to be shut down through the task manager.
