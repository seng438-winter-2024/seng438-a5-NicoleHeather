**SENG 438- Software Testing, Reliability, and Quality**

**Lab. Report \#5 – Software Reliability Assessment**

| Group 23:       |   |
|-----------------|---|
| Jennifer Jay    |   |
| Nicole Heather  |   |
| Nora Melik      |   |
| Creek Thompson  |   |

# Introduction

Software reliability and quality are the parts of the final steps of testing for an application. As a result, we are now shitfting away from unit testing with source code to focusing on the deployment and integration of program. Through the use of these reliability testing tools we can experiment with different models and metrics in order to properly determine if our SUT meets our desired requirements. This kind of testing however is not just for the developers themsevles, but also allows stakeholders and purchasers to undertsand and have input on the kind of thresholds in place for the program. As a result reliability and quiality testing are major parts of testing within software, and provide insights into the determination and risk assesment of deploying a product.

# Assessment Using Reliability Growth Testing 

Result of model comparison (selecting top two models):

Geometric and Littlewood and Varral's Bayesian Reliability were used for the Time Between Failures, and Schneidewind’s Maximum Likelihood and Yamada’s S-Shaped Reliability Growth were used for Failure Count. For Time Between Failures, Littlewood and Varral's Bayesian Reliability fits the data better than Geometric, as the blue line of fit follows the red data points more accurately, indicating that out of the two models, Littlewood and Varral's Bayesian Reliability was more successful in learning and plotting the data. For Failure Count, the models chosen do not reflect a line of best fit, making it more difficult to analyze which model is a better algorithm for the Failure Count data. However, the Schneidewind’s Maximum Likelihood provides more calculated parameters that represent values that can be used to identify different characteristics of the dataset, whereas Yamada’s S-Shaped Reliability Growth only provides Estimate values and Prediction. These models in general may not have been the best possible outcome for representing the data, as they lacked a line of best fit, but they were the only available models in the SRTAT that successfully outputted information when run. Due to more metrics being given for the Schneidewind’s Maximum Likelihood model, it would be the better fit out of the two for Failure Count. Therefore, the selection of the top two models would be Littlewood and Varral's Bayesian Reliability for Time Between Failures and Schneidewind’s Maximum Likelihood for Failure Count.

Result of range analysis (an explanation of which part of data is good for proceeding with the analysis):

To find a suitable range for analysis, it is important to reject outliers that cause deviation from the overall trend of the data to avoid skewing analysis results. In the Geometric analysis graph, we would reject the 3 points above time 4000 and before failure number 110 as they clearly deviate from the exponentially increasing trend of the data. The other points relatively fit the trend so restricting a time period is not crucial. For the Littlewood and Varral's Bayesian Reliability graph, we would remove the 2 points above time 7,000 and a point roughly at (86, 4150) since those deviate the most from the trend. We would limit the time period to start after failure number 5 since there are a couple points prior that skews the beginning of the exponential trend to start closer to time 1,000 when most of the points are closer to time 0. For Schneidewind’s Maximum Likelihood and Yamada’s S-Shaped Reliability Growth graphs, we would reject the outliers above time 7.

Plots for failure rate and reliability of the SUT for the test data provided:

Geometric:

![alt text](Images/Geometric_Data.png)

![alt text](Images/Geometric_Chart.png)

Littlewood and Varral's Bayesian Reliability:

![alt text](Images/LV_Data.png)

![alt text](Images/LV_Chart.png)

Schneidewind’s Maximum Likelihood:

![alt text](Images/SM_Data.png)

![alt text](Images/SM_Chart.png)

Yamada’s S-Shaped Reliability Growth:

![alt text](Images/YS_Data.png)

![alt text](Images/YS_Chart.png)

A discussion on decision making given a target failure rate:

If the observed failure rate does not meet a given target failure rate, we need to decide how feasible it would be to close the gap. A decision should be made based on constraints of resources, time and technical capabilities. The risks of accepting the current failure rate should also be accounted for. If the observed failure rate does meet the failure rate, a discussion on the implications of proceeding with the current system’s reliability should be made.

A discussion on the advantages and disadvantages of reliability growth analysis:

Reliability growth analysis allows users to assess improvements and identify weaknesses through performance tracking so that they can enhance the system based on targeted areas needed for improvement. However, reliability growth data heavily requires failure data. Failure data must be available with limited bias to avoid inaccurate analysis. The analysis also assumes that the system will continuously improve in reliability, however, this is not always the case if underlying issues of the system remain unresolved.

# Assessment Using Reliability Demonstration Chart 

3 plots for MTTFmin, twice and half of it for your test data:

For Dataset CSR2.DAT:

- Standard MTTFmin:

![alt text](Images/Standard_CSR.png)

![alt text](Images/CSR_S.png)

- Double MTTFmin:

![alt text](Images/Double_CSR.png)

![alt text](Images/CSR_D.png)

- Half MTTFmin:

![alt text](Images/Half_CSR.png)

![alt text](Images/CSR_H.png)

For Dataset J1.DAT:

- Standard MTTFmin:

![alt text](Images/Standard_J1.png)

![alt text](Images/J1_S.png)

- Double MTTFmin:

![alt text](Images/Double_J1.png)

![alt text](Images/J1_D.png)

- Half MTTFmin:

![alt text](Images/Half_J1.png)

![alt text](Images/J1_H.png)

Explain your evaluation and justification of how you decide the MTTFmin:

CSR2.DAT:
- Evaluation of Result:

    - X-axis (Number of input events in normalized usage units): This axis represents the input events, likely actions or interactions with the system, normalized to some standard unit.

    - Y-axis (Failure Number): This axis represents the number of failures experienced by the system. Each point on the chart likely corresponds to a certain number of input events and the corresponding number of failures.

    - Points (0.5, 1) and (2, 2) fall within the yellow zone, indicating that for a certain number of input events, the system experienced a relatively low number of failures, which is within an acceptable range.

    - The line becomes almost vertical around 2.5 on the x-axis, suggesting a sudden increase in the number of failures as the number of input events increases beyond this point. This signifies a critical point where the system's failure rate rapidly escalates.

 - MTTFmin Justification: 
    The original dataset lacks a specific time unit for its time-between values, however this is ignorable since they are normalized when plotted into the RDC. First we start with a reasonable assumption on the usage and up-time for MTTF, that way we can cover for a standard, very high and very low MTTFmin when re-calculating and re-making the graph. We must also take into account the tool used and readability when evaluating the output, a value that is too high or too low makes the result difficult to undertsand. For this failure dataset we can assume that each time unit can be equated to a second (for the sake of using SRTAT) and that we have system that functions reasonably. 
    
    From looking at our provided data, our system ran for a total of 61,316 time units, with a total of 129 failures. So lets say that our starting FIO is 129 / 61,316 = 0.002 failures / second. Taking into account the data from our report helps keep the persepective of how the system actually functioned, instead of needing to assume off of no knowledge. This means our MTTFmin is 1.5, doubling it gives; MTTFmin * 2 = 3 and dividing it gives; MTTFmin / 2 = 0.75.

J1.DAT:
- Evaluation of Result:

    - X-axis (Number of input events in normalized usage units): This axis represents the input events, likely actions or interactions with the system, normalized to some standard unit.

    - Y-axis (Failure Number): This axis represents the number of failures experienced by the system. Each point on the chart likely corresponds to a certain number of input events and the corresponding number of failures.

    - The entire chart consistently operates within the red zone, indicating persistent and severe issues with the system's reliability and performance. This suggests chronic failures regardless of the input load.

    - Such consistent operation in the red zone signifies potential systemic issues such as overload, design flaws, maintenance problems, environmental factors, aging infrastructure, or inadequate testing/quality assurance.

- MTTFmin Justification:
    Very similarly to the previous dataset, we lack any specific time units to use as a benchmark. So we will be approaching this data set in the same way. This includes taking into account the data that we already have, and ensuring that we still make reasonable assumptions so that evaluating the resulting charts are made clear. For the sake of consistancy, we are not changing any risk values or discrimination factors. 

    The failure count data gives us a slighly different view into the functioning of this system. The time intervals are always set in singles slices, whereas the counting of failures resets each time. After running for a total of 62 time units, we have a total of 150 failures. This leaves us with a failure rate of 150 / 62 = 2.4 failures per time unit. Setting this as our FIO, we have a MTTFmin value of around 14. Now much like before we can both double (MTTFmin * 2 = 28), and half this (MTTFmin / 2 = 7). 

Comparison:

  - While CSR2 shows fluctuations between the yellow and red zones, suggesting occasional critical points, the J1 dataset remains entirely in the red zone, indicating ongoing and severe reliability issues.

  - The transition from occasional critical points (CSR2) to continuous operation within the red zone (J1) underscores a worsening condition of the system's reliability and performance over time.

  - Addressing the issues highlighted by the J1 data would likely require urgent and comprehensive measures to diagnose root causes and implement appropriate corrective actions to improve the system's reliability and performance.

A discussion on the advantages and disadvantages of RDC:

- Advantages: RDC is an efficient way to provide information on the performance and reliability of a system, it is also easily understood as the chart itself makes very clear what the data represents. This method also allows for input of not just the developers, but any stakeholders as well. This rask managements is flexible, allowing multiple changes to both the accept and reject lines as well as the ability to experiment and compare MTTFmin and FIO values. Finally the data we used for the creation of these charts was minimal, meaning that we do not need an a large amount of data to analyze our system as the chart makes the visualization relativley easy. 

- Disadvantages: RDC is a tool that only able to tell us if a system meets an acceptable level of requirements. The information is provides us is not exact, and can only give a more simplified overview of te SUT. Another issue is the lack of specific sytem information. For example, the data used for the charts above lacked any specific data regarding the kinds of failiures nor how these failures happened. This could be serious issue in an actual development enviroment where we need to fix these issues in order for the system to meet requiremtns. RDC in no way can tell us specifically where we can improve our code, only that we meet our specifications or not.

# Comparison of Results

Reliability Growth Testing:
The results for Part 2 involved finding and comparing the top two models for each dataset by using the SRTAT tool, where Geometric and Littlewood and Varral's Bayesian Reliability were the models applied to the Time Between Failures data, and the Schneidewind’s Maximum Likelihood and Yamada’s S-Shaped Reliability Growth models were applied to the Failure Count. As analyzed in Part 1, it was found that Littlewood and Varral's Bayesian Reliability was the better model for Time Between Failures, and for Failure Count, it was found that Schneidewind’s Maximum Likelihood was the better model. The range analysis also served in inspecting the actual data used for these models and how removing outliers could improve the models’ behavior. 

RDC:
The aftermath of Part 3 aims to more accurately depict the reliability and quality of the theoretical SUT's and the corresponding data. RDC was able to provide a a more visual representation of the system, as a results we can see more clearly the certain decisions a developer or stockholder must make in regards to their project. Consistantly, it seems that our theorectical program for both of our datasets do not perform well, as they all have enough failures to eventually make it to the rejected zone. Even when we half the MTTFmin values we get from the dataset, we still have issues with the failure rate. Despite this failure in quality, this does show that this method of quality assurance is able to reliably convey any issues in the performance of the system. 

Comparison:
Both methods made where done on the same datasets, and as such both methods made clear that our theoretcial SUT is not very reliable. These interpretations can change if we take into account the different models, outliers rejected and how we decide MTTFmin, as these factors can individually change our results. In the case of Reliability Growth Testing, we needed to make the decisions to restrict our time and outlier values in order to acheive the best results possible for analysis. So the results obtained from RGT lean more toward analysis and decision making for determining the best model to represent a system. RDC is more simple in comparison, but also more flexible due to the risk and discrimination parameters being changible, meaning developers have more of a say in the aftermath of any calculations.

# Discussion on Similarity and Differences of the Two Techniques

Similarities:
Both techniques aim to assess the reliability of a system over a specified period of time and rely on statistical analysis from collected data to do so. Another throughline of both these methods are that they are mathmatical and visual representations of a systems quality. Although the output looks different compared to each other, they work with exactly the same data. This makes it so that using both methods can be a reliable way to test and quantify the quality of the SUT. Both methods require decision making on the side of the developers, whether that would be finding the best model, failure rate, risk values or MTTFmin. Trend analysis is also common between these methods, using charts and graphs to visualize failure data makes these strategies more efficient and undertsandible compared to reading chart data or going through a bug report.

Differences:
Starting with RDC, it is a far more simple method in execution and analysis compared to RGT. The interpretation of the chart results are easily understood, being able to change the the risk parameters make RDC better for development that has changes in management or partners. On the other hand, RGT requires slighly more analysis and decision making in order to properly interpret the results of the system. RTG is able to predict possible failures based on the data given to the model as well as its trend analysis, this allows developers to extrapolate information for future test analysis. On the other hand, RDC deals with data only in the present, its gives no further information other than how the SUT performs now and not how it can perform on longer intervals. Reliability Growth Testing focuses on identifying weaknesses in the system to improve overall reliability over time and involves continuously testing and improving a system during its development lifecycle. On the other hand, Reliability Demonstration Chart is typically used for products that are already developed, involving a series of tests of simulations to demonstrate that the system meets specified reliability requirements.

# How the team work/effort was divided and managed

The planning of the stages where all done together, individually each member had some times to familiarize themsevles with the specific tools being used. Then all member met together to follow though on the execution and analysis of the ouput of our reliablility testing tools. This was done all together unlike the previous labs because of the lack of coding required, as well as wanting the analysis and comparison to be freely disscused and argued among everyone.

As for the lab report itself, all members contirbuted to each section in some way. This includes writing, editing and quality assurance. This way we could have a consistent report while still distributing work around evenly between each member. 

# Difficulties encountered, challenges overcome, and lessons learned

Originally, there where some issues in regards to the functioning of the tools used. This resulted in more time needing to be taken to get used to our tools as well a figure out how to approach the analysis and comparison of our results. The instructions where also unclear on how the analysis should have been constructed so we did our best to supply as much information as possible in our report, in order to justify our findings.

However this lab was an insightful look into the uses and procedure of both reliability growth testing and the demonstration chart. Being able to look at the many different uses and models used for the visualization of reliability and quality was benificial in our understanding of software testing.

# Comments/feedback on the lab itself

This lab had some issues with the instructions and tools given. Although followable, the instructions in the lab document lacked any details. It is difficult, especially for a subjective task, to ensure that the process of choosing models, interpreting graphs and setting new values are all done somwhat correctly. It would help to have a more concrete explanation within the lab itself than have to read over the large amount of documentation given for each tool we might use, as it is hard to discern what information is relevant.\

There is also some issues with the tool provided themselves. These tools look very different than what was provided in the document, and the data that was given in order to complete the section for part 1 was unusable. We where able to test and learn how to use these tool using the sample data provided, but that data was not the target for this lab. There is also an issue with the computing time and errors with SRTAT specifically, where some models and formulas gave unuexplanable errors, or would execute seemingly forever and needed to be shut down through the task manager.
